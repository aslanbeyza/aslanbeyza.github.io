---
title: "BÃ¼yÃ¼k Dil Modelleri ile Generative AI"
date: 2025-04-23 12:00:00 +0300
categories: [BÃ¼yÃ¼k Dil Modelleri ile Generative AI]
tags: [kiÅŸisel, giriÅŸ]
---

## TRANSFORMER

TransformerÄ± anlamak iÃ§in [**encoder**] ve [**decoder**]  , [**Dikkat MekanizmasÄ±**]  , [**Transfer Learning teknikleri**]ni  anlamak gerekir. Hemen Encoder-Decoder ile baÅŸlayalÄ±m.

### Encoder 
Transformerdan Ã¶nce LSTM gibi RNN gibi mimariler doÄŸa dil iÅŸlemede Ã§ok Ã¶nemliydi.Bu mimarilerin en bÃ¼yÃ¼k Ã¶zelliÄŸi aÄŸ baÄŸlantÄ±larÄ±na geri bildirimler saÄŸlamasÄ±ydÄ±.


![Desktop View](/assets/img/RNNYapÄ±sÄ±.png){: .w-75 .mx-auto d-block }


Ekranda bir RNN Mimarisini gÃ¶rÃ¼yorsunuz bu RNN hÃ¼cresini aÃ§tÄ±ÄŸÄ±nÄ±zda bu ÅŸekilde bir yapÄ± sizi karÅŸÄ±lar.

Dikkat ederseniz 
 
 - 1. girdi ilk hÃ¼creye giriyor ve bu hÃ¼creden bir Ã§Ä±ktÄ± elde ediliyor.
 - 2. Daha sonra ikinci girdi ile beraber birinci girdinin Ã§Ä±ktÄ±sÄ± hÃ¼creye giriyor. Her hÃ¼creye bir Ã¶nceki hÃ¼crenin Ã§Ä±ktÄ±sÄ±da veriliyor.Buna **Hidden State** denir.Her bir hÃ¼crede bir Ã¶nceki Ã§Ä±ktÄ±da kullanÄ±ldÄ±ÄŸÄ± iÃ§in aÄŸfa bir hafÄ±za oluÅŸuyor. bu RNN Mimarileri zaman serileri, ses iÅŸleme gibi doÄŸal dil iÅŸleme gÃ¶revlerinde sÄ±k sÄ±k  kullanÄ±ldÄ±. RNN mimarileri ile metinde Ã¼retebilirsiniz.

![Desktop View](/assets/img/EncoderDecoder.png){: .w-75 .mx-auto d-block }


RNN Mimarileri Encoder, Decoder veya Seq2seq  gÃ¶revler iÃ§inde kullanÄ±ldÄ±. Bu gÃ¶revlerde modele bir dizi veriliyor ve Ã§Ä±ktÄ± olarak  bir dizi elde edilmek isteniyor.

**Encoder** girdi dizilerini sayÄ±sal temsillere Ã§evirir.
**Decoder** Bu veriler decoder kÄ±smÄ±na verilir. Decoder ise Ã§Ä±ktÄ± dizisini Ã¼retir.


![Desktop View](/assets/img/EncoderDecoderArray.png){: .w-75 .mx-auto d-block }


Yandaki resimde Transformers are great! cÃ¼mlesinin almancasÄ± mevcut resimdede gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere modele bir dizi giriyor ve bir dizi Ã§Ä±kÄ±yor.
Encoder blok bu girdi dizilerini sayÄ±sal temsillere Ã§eviriyor. State kÄ±smÄ±nÄ± bir zaman adÄ±mÄ±nda ortaya Ã§Ä±kan veri olarak dÃ¼ÅŸÃ¼nebilirsiniz.Son statedeki bilgiler decoder bloÄŸuna geÃ§iyor ve bu blokta bir dizi Ã¼retiliyor.Bu mimarinin zayÄ±f tarafÄ± encoder bloÄŸundaki son gizli state kÄ±smÄ±na bir dar boÄŸazÄ±n oluÅŸmasÄ± uzun bir metin dÃ¼ÅŸÃ¼nÃ¼n ve bu metindeki tÃ¼m kelimeler iÅŸleniyor.Bu kelimeler iÅŸlene iÅŸlene sonlara doÄŸru bu veriler birikiyor ve en son state decoder bloÄŸuna geÃ§erken bir dar boÄŸaz oluÅŸuyor iÅŸte bu sorunu Ã§Ã¶zmek iÃ§in [**Attention MekanizmasÄ±**] kullanÄ±lÄ±yor. Yani Dikkat YaklaÅŸÄ±mÄ± mekanizmasÄ± gerÃ§ekleÅŸtirildi.
![Desktop View](/assets/img/attention.png){: .w-75 .mx-auto d-block }


Bu dikkat mekanizmasÄ± decoder kÄ±smÄ±nÄ±n encoder kÄ±smÄ±ndaki tÃ¼m statelere ulaÅŸmasÄ±nÄ± saÄŸlÄ±yor.Bu Dikkat MekanizmasÄ± Modern Yapay Zeka Mimarisinde kilit rol oynadÄ±.Dikkat MekanizmasÄ±nÄ±n arkasÄ±nda yatan ana fikir gizli state kullanÄ±mÄ±. Encoder sondaki gizli state kÄ±smÄ±nÄ± decoder kÄ±smÄ±na veriyordu.Dikkat mekanizmasÄ±nda her bir adÄ±m iÃ§in bir gizli state Ã¼retir.Her bir RNN hÃ¼cresi bir state Ã§Ä±ktÄ±lÄ±yor ama aynÄ± zaman adÄ±mÄ±ndaki tÃ¼m stateler decoder iÃ§in bÃ¼yÃ¼k bir girdi oluÅŸturabilir.DolayÄ±sÄ±yla bazÄ± mekanizmalarÄ±n hangi statelerin daha Ã¶nemli olduÄŸunu belirlemesi gerekir.Ä°ÅŸte tam burada attention yani dikkat mekanizmasÄ± devreye girer 
![Desktop View](/assets/img/attention2.png){: .w-75 .mx-auto d-block }


Bu mekanizma her bir state iÃ§in farklÄ± bÃ¼yÃ¼klÃ¼kteki aÄŸÄ±rlÄ±klar atar. BÃ¶ylece dar boÄŸaz sorunu Ã§Ã¶zÃ¼lmÃ¼ÅŸ olur.

ğŸ’• Åimdi ise attention Ä± anlamak iÃ§in bir Ã§eviri gÃ¶revini ele alalÄ±m  
![Desktop View](/assets/img/Ã§evirigÃ¶revi.png){: .w-75 .mx-auto d-block }


Burada ingilizce bir cÃ¼mle fransÄ±zcaya Ã§evrilmiÅŸ burada her bir piksel bir aÄŸÄ±rlÄ±ÄŸÄ± temsil eder.Attention kelimeler arasÄ±ndaki iliÅŸkiyi Ã¶ÄŸrenir ve hangi kelimeler arasÄ±nda iliÅŸki var hangi kelimeler arasÄ±nda iliÅŸki yok olduÄŸunu belirler.Buna gÃ¶re aÄŸÄ±rlÄ±klarÄ± atar. Ä°ngilizce cÃ¼mlede dikkat edin bir area keilmesi mevcut frasÄ±zcada bu kelimeye karÅŸÄ±lÄ±k zone kelimesi geliyor.Bu iki kelimenin cÃ¼mledeki yerleri farklÄ± dimi biri 6. sÄ±rada diÄŸeri 5. sÄ±rada iÅŸte attention bu iki kelimenin iliÅŸkili olduÄŸunu Ã¶ÄŸrenir.Ancak ÅŸÃ¶yle eksiklikler var biz modele dizi veriyoruz dimi ve Ã§Ä±ktÄ± olarak dizi alÄ±yoruz Bu dizideki kelimeler tek tek iÅŸleniyor.Ä°ÅŸte bu iÅŸleme sÄ±rasÄ±nda hesaplamalar sÄ±rayla yapÄ±lÄ±yor.yani bir iÅŸlem yapÄ±lÄ±yor o bitiyor sonra diÄŸer iÅŸlem yapÄ±lÄ±yor.Ä°ÅŸte bu iÅŸlemler paralel ÅŸekilde yapÄ±lamÄ±yor.Bu problemin Ã§Ã¶zÃ¼lebilmesi iÃ§inde transformer mimarisi geliÅŸtiriliyor.
![Desktop View](/assets/img/Transformer.png){: .w-75 .mx-auto d-block }


Ekranda gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi bu mimari attention yerine self attention yaklaÅŸÄ±mÄ±nÄ± kullandÄ±.Normalde sinir aÄŸlarÄ± katmanlardan oluÅŸur dimi bu katmanlarda nÃ¶ronlar yani birimler vardÄ±r.Attention her bir birimdeki statelerle Ã§alÄ±ÅŸÄ±yordu.Self attention ise aynÄ± katmanda bulunan tÃ¼m birimlerdeki stateler Ã¼zerinde Ã§alÄ±ÅŸÄ±r.
Resimdede gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi hem encoder hem decoder kendi self attention mekanizmasÄ± var. Bu mekanizmalarda RNN lerin kullanÄ±ldÄ±ÄŸÄ± tekrarlÄ± sinir aÄŸÄ± yerine ileri beslemeli sinir aÄŸÄ± kullanÄ±ldÄ±.Bu mimari RNN mimarisinden daha hÄ±zlÄ± eÄŸitildi.Bu teknik Fatih in Ä°stanbulu fethi gibi bir Ã§aÄŸ aÃ§tÄ± bir Ã§aÄŸ kapattÄ±.Bu mimari kullanÄ±larak doÄŸal dil iÅŸlemede birÃ§ok model geliÅŸtirildi.Orijinal transformer mimarisi bÃ¼yÃ¼k miktarda veri ile Ã§ok gÃ¼Ã§lÃ¼ bilgisayarlar kullanÄ±larak sÄ±fÄ±rdan eÄŸitildi . Ancak birÃ§ok doÄŸal dil iÅŸleme uygulamasÄ± iÃ§in pratikte etiketli veri bulmak oldukÃ§a zordur.AyrÄ±ca bu modelleri eÄŸitmek iÃ§in gÃ¼Ã§lÃ¼ bilgisayarlar gereklidir.
Ã‡oÄŸu kiÅŸide maalesef bÃ¶yle bilgisayarlar yok.Ä°ÅŸte bu sorunlarÄ± Ã§Ã¶zmek iÃ§in[**Transfer Learning**] yaklaÅŸÄ±mÄ± geliÅŸtirildi. 


![Desktop View](/assets/img/TransferLearning.png){: .w-75 .mx-auto d-block }


Transfer Learning kÄ±saca daha Ã¶nceden eÄŸitilmiÅŸ bir modeli alÄ±p onu kendi projenize adapte etmenizdir.SÄ±fÄ±rdan model eÄŸitmemize gerek yok Ã¶rneÄŸin ResNet gibi bir sinir aÄŸÄ±nÄ± alÄ±p kendi veriniz iÃ§in onu fine-tuning edebilirsiniz.


![Desktop View](/assets/img/finetuning.png){: .w-75 .mx-auto d-block }


BÃ¼yÃ¼k dil modellerinde fine-tune kavramÄ± Ã§ok sÄ±k duyulur.Bu teknik eÄŸitilmiÅŸ bir modelin hiperparametreleri ile oynamak olarak dÃ¼ÅŸÃ¼nebilirsin.Yani bir gitarÄ± akort etmek gibi 
![Desktop View](/assets/img/SupervisedLearning.png){: .w-75 .mx-auto d-block }


Eskiden A alanÄ±ndaki bir problem iÃ§in ayrÄ± bir model Ã¼retilirdi B alanÄ±ndaki bir problem iÃ§inde baÅŸka bir model eÄŸitilirdi.Her problem iÃ§in ayrÄ± ayrÄ± modeller eÄŸitilirdi.
![Desktop View](/assets/img/TransferLearning2.png){: .w-75 .mx-auto d-block }


Transfer Learning tekniÄŸi ile mimariler gÃ¶vde ve baÅŸ olarak ayrÄ±ldÄ±. Bir proje yaparken gÃ¶vdeyi aynen alÄ±rsÄ±nÄ±z baÅŸ kÄ±smÄ±nÄ± probleminize gÃ¶re ayarlarsÄ±nÄ±z. BÃ¶ylece A alanÄ±ndaki eÄŸitilen bir gÃ¶rev iÃ§in modeli B alanÄ± iÃ§inde kullanabilirsiniz.Ã–zellikle Computer Vision alanÄ±nda Transfer Learning kullanÄ±ldÄ±.


Modeller ilk olarak 

![Desktop View](/assets/img/Animals.png){: .w-75 .mx-auto d-block }


Modeller ilk olarak imageNet gibi bÃ¼yÃ¼k veri setlerinde eÄŸitildi Bu sÃ¼rece pre-training yani Ã¶n eÄŸitim denilir.Bu eÄŸitimde  model, kÃ¶ÅŸe, renk gibi temel Ã¶zellikleri Ã¶ÄŸrenir.Ã–rneÄŸin kÃ¶ÅŸeler her resimde var dimi bunlar temel feature yani Ã¶z niteliklerdir.Ä°ÅŸte bu Ã¶n eÄŸitimli modeli fine-tune edererk Ã§iÃ§ek sÄ±nÄ±flandÄ±rma gibi gÃ¶revler iÃ§inde kullanabiliriz.


![Desktop View](/assets/img/resnet.png){: width="972" height="589" .w-75 .normal}


Ã–rneÄŸin kedi kÃ¶pek sÄ±nÄ±flandÄ±rma gibi bir probleminiz var  


![Desktop View](/assets/img/resnet2.png){: .w-75 .mx-auto d-block}


iÅŸte bu problemi Ã§Ã¶zmek iÃ§in resNetin son katmanÄ±nÄ± kendi probleminiz iÃ§in ayarlamanÄ±z yeterli Biraz ilginÃ§ gelecek ama bu Transfer Learning kullanÄ±larak inÅŸa edilen modeller sÄ±fÄ±rdan eÄŸitilen modellerden daha iyi performans gÃ¶sterdi.

![Desktop View](/assets/img/ConputerVision.png){: width="972" height="589" .w-75 .normal}


Transfer Learning Computer Vision yani Bilgisayar gÃ¶rÃ¼sÃ¼ alanÄ±ndaki gÃ¶revler iÃ§in baÅŸarÄ±lÄ± bir ÅŸekilde uygulandÄ±.Fakat 2017 yÄ±lÄ±ndan Ã¶nce bu teknik doÄŸal dil iÅŸleme iÃ§in tam uygulamanadÄ±.O zamanlar doÄŸal dil iÅŸleme projeleri yapmak iÃ§in bÃ¼yÃ¼k miktarda etiketli veri gerekiyordu.BÃ¼yÃ¼k miktarda veriniz olsa bile Computer Visiondaki modellerin baÅŸarÄ±sÄ± Nlp gÃ¶revleri iÃ§in elde edilemiyordu.


![Desktop View](/assets/img/Bert.png){: width="972" height="589" .w-75 .normal}


2017 yÄ±lÄ±ndan sonra yeni bir yaklaÅŸÄ±mla transfer learning  doÄŸal dil iÅŸleme iÃ§inde kullanÄ±labilir hale getirildi.Bu denetimsiz Ã¶n iÅŸleme ile elde edilen feature lar kullanÄ±larak yapÄ±ldÄ±.Bu teknikle metin sÄ±nÄ±flandÄ±rma problemleri yÃ¼ksek doÄŸrulukla Ã§Ã¶zÃ¼ldÃ¼. 



![Desktop View](/assets/img/user.png){: width="972" height="589" .w-75 .normal}


Daha sonra bu tekniÄŸi uygulamak iÃ§in ***ULMFÄ°T*** kÃ¼tÃ¼phanesi geliÅŸtirildi.Bu kÃ¼tÃ¼phane transfer learningi 3 aÅŸama ile uyguladÄ±.

- 1. aÅŸamada **pretraning** yapÄ±ldÄ±.Bu adÄ±mda gelecek kelimelerin tahmini Ã¶nceki kelimelere dayanarak yapÄ±ldÄ± bu iÅŸleme dil modelleme denildi.Bu yaklaÅŸÄ±mda etiketli veriye ihtiyaÃ§ duyulmadÄ±.Yani wikipedia gibi kaynaklardan bolca yararlanÄ±ldÄ±.


- 2. **Adaptasyon**  diyelim ki modeli wikipedia Ã¼zerinde eÄŸittiniz Bu modeli film yorumlarÄ± iÃ§eremn IMDB ye adapte edebilirsiniz.
Bu aÅŸamada dil modellemeyi kullanÄ±yor.Fakat model hedef deki gelecek kelimeyi tahmin eder.


- 3. **Fine Tuning** son aÅŸama yani ince ayar dil modeli bir gÃ¶rev iÃ§in sÄ±nÄ±flandÄ±rma katmanÄ± ile Fine-tuning edilir. ULMFÄ°T kÃ¼tÃ¼phanesi Transfer Learning ve preTraining tekniklerini doÄŸal dil iÅŸlemede kullanÄ±lmasÄ±nÄ± saÄŸladÄ±.


![Desktop View](/assets/img/EvolutionofLLM.png){: width="972" height="589" .w-75 .normal}


2018 de Transfer Learning ile self attention Ä± birleÅŸtiren 2 Ã¶nemli transfer learning geliÅŸtirildi.Bunlar Gpt ve Bert modelleriydi.


![Desktop View](/assets/img/gptbert.png){: .w-75 .mx-auto d-block }


gpt transformer mimarisinin decoder bloÄŸunu kullanÄ±rken Bert transformer mimarisinin encoder bloÄŸunu kullandÄ±.Bert maskelenmiÅŸ dil modelidir.Bu modeller bir metindeki maskelenmiÅŸ rastgele kelimeleri tahmin eder.


![Desktop View](/assets/img/sentence.png){: .w-75 .mx-auto d-block }


iÅŸte model cÃ¼mledeki mask ile kapatÄ±lmÄ±ÅŸ kelimeleri tahmin eder.Bert ve gpt ile yeni bir transformer Ã§aÄŸÄ± baÅŸlatÄ±ldÄ±.


![Desktop View](/assets/img/Huggingface.png){: width="972" height="589" .w-75 .normal}


iÅŸte Hugging Face tarafÄ±ndan geliÅŸtirilen Transformers kÃ¼tÃ¼phanesi ile bÃ¼yÃ¼k Dil Mpodelleri iÃ§in bir standart getirildi. Bu kÃ¼tÃ¼phane 


![Desktop View](/assets/img/framework.png){: width="972" height="589" .w-75 .normal}


resimdeki frameworkleri desteklemektedir.Bunun anlamÄ± Transformerlarla Ã§alÄ±ÅŸmak iÃ§in bu 3 frameworkÃ¼de kullanabilirsiniz.


![Desktop View](/assets/img/sorucevap.png){: width="972" height="589" .w-75 .normal}


AyrÄ±ca transformers gÃ¶reve Ã¶zgÃ¼ mimarilerde sundu. Yani metin sÄ±nÄ±flandÄ±rma veya soru cevap gibi projelerizi yapmak iÃ§in bu mimarilerin baÅŸ tarafÄ±nÄ± fine-tune etmek yeterli olacaktÄ±r.BÃ¶ylece kÄ±sa zamanda mdoelinizi eÄŸitip probleminizi Ã§Ã¶zebilirsiniiz.
KÄ±saca BÃ¼yÃ¼k dil modelleri ile yapÄ±lan Ã§alÄ±ÅŸmalar hÄ±zlandÄ± ve gerÃ§ek hayat problemlerinin Ã§Ã¶zÃ¼mÃ¼ kolaylaÅŸtÄ±.



